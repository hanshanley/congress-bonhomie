{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "# Retrieve Congressional Speeches (GovInfo)\n",
        "\n",
        "This notebook fetches speeches from the Congressional Record via the GovInfo API.\n",
        "\n",
        "- Set a date range and your `GOVINFO_API_KEY` (env var or prompt).\n",
        "- The code paginates through Daily Edition issues (CREC), fetches granules, downloads XML/TXT, and extracts individual `<speaking>` blocks.\n",
        "- Output is written to JSONL (one speech per line), with optional CSV export.\n",
        "\n",
        "Note: Fetching all history is very large. Start with a small range, confirm results, then scale up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [

      ],
      "source": [
        "# Configuration and imports\n",
        "import os, time, sys, json, re\n",
        "from datetime import date, timedelta\n",
        "from typing import Iterator, Dict, Any, List, Optional, Tuple\n",
        "\n",
        "try:\n",
        "    import requests\n",
        "except ModuleNotFoundError:\n",
        "    # Install requests if missing (uncomment if running in a clean kernel)\n",
        "    # %pip install requests\n",
        "    raise\n",
        "\n",
        "BASE = \"https://api.govinfo.gov\"\n",
        "API_KEY = os.getenv('GOVINFO_API_KEY')\n",
        "if not API_KEY:\n",
        "    try:\n",
        "        from getpass import getpass\n",
        "        API_KEY = getpass('Enter GOVINFO_API_KEY: ').strip()\n",
        "    except Exception:\n",
        "        API_KEY = input('Enter GOVINFO_API_KEY: ').strip()\n",
        "\n",
        "assert API_KEY, 'A GOVINFO_API_KEY is required. Get one from https://api.govinfo.gov/docs/'\n",
        "\n",
        "# Tuning\n",
        "RATE_DELAY = 0.2  # seconds between API calls to be polite\n",
        "PAGE_SIZE = 100   # max page size supported by GovInfo\n",
        "\n",
        "def _get(path: str, params: Optional[Dict[str, Any]] = None, stream: bool = False):\n",
        "    params = dict(params or {})\n",
        "    params['api_key'] = API_KEY\n",
        "    for attempt in range(6):\n",
        "        r = requests.get(BASE + path, params=params, timeout=60, stream=stream)\n",
        "        if r.status_code in (429, 502, 503, 504):\n",
        "            time.sleep(min(2 ** attempt, 10))\n",
        "            continue\n",
        "        r.raise_for_status()\n",
        "        return r\n",
        "    r.raise_for_status()\n",
        "\n",
        "def iter_crec_packages(start_date: str, end_date: str, page_size: int = PAGE_SIZE) -> Iterator[Dict[str, Any]]:\n",
        "    \"\"\"Yield Daily Edition (CREC) packages between dates (inclusive).\n",
        "    Dates are YYYY-MM-DD.\n",
        "    \"\"\"\n",
        "    offset = 0\n",
        "    while True:\n",
        "        resp = _get('/collections/CREC', params={\n",
        "            'startDate': start_date,\n",
        "            'endDate': end_date,\n",
        "            'pageSize': page_size,\n",
        "            'offset': offset,\n",
        "        }).json()\n",
        "        items = resp.get('packages', []) or []\n",
        "        if not items:\n",
        "            break\n",
        "        for p in items:\n",
        "            yield p\n",
        "        if len(items) < page_size:\n",
        "            break\n",
        "        offset += page_size\n",
        "        time.sleep(RATE_DELAY)\n",
        "\n",
        "def iter_granules(package_id: str, page_size: int = PAGE_SIZE) -> Iterator[Dict[str, Any]]:\n",
        "    offset = 0\n",
        "    while True:\n",
        "        resp = _get(f'/packages/{package_id}/granules', params={\n",
        "            'pageSize': page_size,\n",
        "            'offset': offset,\n",
        "        }).json()\n",
        "        items = resp.get('granules', []) or []\n",
        "        if not items:\n",
        "            break\n",
        "        for g in items:\n",
        "            yield g\n",
        "        if len(items) < page_size:\n",
        "            break\n",
        "        offset += page_size\n",
        "        time.sleep(RATE_DELAY)\n",
        "\n",
        "def get_granule_summary(package_id: str, granule_id: str) -> Dict[str, Any]:\n",
        "    return _get(f'/packages/{package_id}/granules/{granule_id}/summary').json()\n",
        "\n",
        "def fetch_granule_text(package_id: str, granule_id: str) -> Tuple[Optional[str], Dict[str, Any]]:\n",
        "    \"\"\"Return (text, summary). Prefers XML, falls back to TXT/HTML.\n",
        "    \"\"\"\n",
        "    summary = get_granule_summary(package_id, granule_id)\n",
        "    dl = summary.get('download') or {}\n",
        "    url = dl.get('xmlLink') or dl.get('txtLink') or dl.get('htmLink') or dl.get('htmlLink')\n",
        "    if not url:\n",
        "        return None, summary\n",
        "    # Some links are direct to govinfo.gov content and do not require the API key\n",
        "    r = requests.get(url, timeout=90)\n",
        "    r.raise_for_status()\n",
        "    return r.text, summary\n",
        "\n",
        "def compact_whitespace(s: str) -> str:\n",
        "    # Avoid regex/backslashes to keep notebook JSON simple\n",
        "    return ' '.join((s or '').split())\n",
        "def extract_speeches_from_xml(xml_text: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Parse CREC XML and extract <speaking> blocks.\n",
        "    Returns a list of dicts with keys: speaker, bioguide_id, text.\n",
        "    If no <speaking> found, falls back to paragraphs as a single block.\n",
        "    \"\"\"\n",
        "    import xml.etree.ElementTree as ET\n",
        "    speeches: List[Dict[str, Any]] = []\n",
        "    try:\n",
        "        root = ET.fromstring(xml_text)\n",
        "    except ET.ParseError:\n",
        "        return speeches\n",
        "\n",
        "    def tagname(el):\n",
        "        return el.tag.split('}')[-1]\n",
        "\n",
        "    for node in root.iter():\n",
        "        if tagname(node) == 'speaking':\n",
        "            speaker = node.attrib.get('speaker') or node.attrib.get('speaker_name') or node.attrib.get('who') or ''\n",
        "            bioguide = (node.attrib.get('bioGuideId') or node.attrib.get('bioguide_id') or\n",
        "                        node.attrib.get('bioGuideID') or node.attrib.get('bioguideId') or '')\n",
        "            text = compact_whitespace(''.join(node.itertext()))\n",
        "            if text:\n",
        "                speeches.append({\n",
        "                    'speaker': speaker,\n",
        "                    'bioguide_id': bioguide,\n",
        "                    'text': text,\n",
        "                })\n",
        "\n",
        "    if not speeches:\n",
        "        # Fallback: collect paragraphs\n",
        "        paras: List[str] = []\n",
        "        for node in root.iter():\n",
        "            if tagname(node) == 'p':\n",
        "                t = compact_whitespace(''.join(node.itertext()))\n",
        "                if t:\n",
        "                    paras.append(t)\n",
        "        if paras:\n",
        "            speeches.append({'speaker': '', 'bioguide_id': '', 'text': '\n\n'.join(paras)})\n",
        "\n",
        "    return speeches\n",
        "\n",
        "def parse_page_from_granule_id(granule_id: str) -> Optional[str]:\n",
        "    # Example: CREC-2024-09-05-pt1-PgS1234-2 -> PgS1234-2\n",
        "    m = re.search(r'(Pg[SH][0-9]+(?:-[0-9]+)?)', granule_id or '')\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "print('Config ready.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [

      ],
      "source": [
        "# Driver: set date range and output\n",
        "# Tip: Start with a small range to validate, then expand.\n",
        "END = date.today()\n",
        "START = END - timedelta(days=1)  # change to desired range\n",
        "start_date = START.isoformat()\n",
        "end_date = END.isoformat()\n",
        "\n",
        "OUTPUT_DIR = 'data'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "JSONL_PATH = os.path.join(OUTPUT_DIR, f'speeches_{start_date}_to_{end_date}.jsonl')\n",
        "CSV_PATH = os.path.join(OUTPUT_DIR, f'speeches_{start_date}_to_{end_date}.csv')\n",
        "\n",
        "max_packages: Optional[int] = None   # e.g., 5 to limit during testing\n",
        "max_granules_per_package: Optional[int] = None  # e.g., 50 to limit during testing\n",
        "\n",
        "print(f'Fetching CREC packages from {start_date} to {end_date}...')\n",
        "count_packages = 0\n",
        "count_granules = 0\n",
        "count_speeches = 0\n",
        "\n",
        "with open(JSONL_PATH, 'w', encoding='utf-8') as out:\n",
        "    for p in iter_crec_packages(start_date, end_date):\n",
        "        package_id = p.get('packageId')\n",
        "        pkg_date = p.get('dateIssued')\n",
        "        if not package_id:\n",
        "            continue\n",
        "        count_packages += 1\n",
        "        print(f'Package {count_packages}: {package_id} ({pkg_date})')\n",
        "        granules_seen = 0\n",
        "        for g in iter_granules(package_id):\n",
        "            granule_id = g.get('granuleId')\n",
        "            chamber = (g.get('granuleClass') or '').upper()  # HOUSE / SENATE / EXTENSIONS\n",
        "            if not granule_id:\n",
        "                continue\n",
        "            granules_seen += 1\n",
        "            if max_granules_per_package and granules_seen > max_granules_per_package:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                text, summary = fetch_granule_text(package_id, granule_id)\n",
        "            except Exception as e:\n",
        "                print(f'  - Failed to fetch {granule_id}: {e}')\n",
        "                continue\n",
        "            if not text:\n",
        "                continue\n",
        "            speeches = extract_speeches_from_xml(text)\n",
        "            page = parse_page_from_granule_id(granule_id)\n",
        "            title = (summary.get('title') or g.get('title') or '').strip()\n",
        "\n",
        "            for sp in speeches:\n",
        "                rec = {\n",
        "                    'date': pkg_date,\n",
        "                    'package_id': package_id,\n",
        "                    'granule_id': granule_id,\n",
        "                    'chamber': chamber,\n",
        "                    'page': page,\n",
        "                    'title': title,\n",
        "                    **sp,\n",
        "                }\n",
        "                out.write(json.dumps(rec, ensure_ascii=False) + '\n')\n",
        "                count_speeches += 1\n",
        "\n",
        "            count_granules += 1\n",
        "            if count_granules % 25 == 0:\n",
        "                print(f'  - Processed {count_granules} granules, {count_speeches} speeches so far...')\n",
        "            time.sleep(RATE_DELAY)\n",
        "\n",
        "        if max_packages and count_packages >= max_packages:\n",
        "            break\n",
        "\n",
        "print(f'Done. Packages: {count_packages}, granules: {count_granules}, speeches: {count_speeches}.')\n",
        "print(f'JSONL written: {JSONL_PATH}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [

      ],
      "source": [
        "# Optional: convert JSONL to CSV for quick analysis\n",
        "import csv, json\n",
        "\n",
        "def jsonl_to_csv(jsonl_path: str, csv_path: str, field_order: Optional[List[str]] = None):\n",
        "    rows = []\n",
        "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    if not rows:\n",
        "        print('No rows to write.')\n",
        "        return\n",
        "    if not field_order:\n",
        "        # reasonable default order\n",
        "        field_order = [\n",
        "            'date','chamber','speaker','bioguide_id','title','page','package_id','granule_id','text'\n",
        "        ]\n",
        "    with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        w = csv.DictWriter(f, fieldnames=field_order, extrasaction='ignore')\n",
        "        w.writeheader()\n",
        "        for r in rows:\n",
        "            w.writerow(r)\n",
        "    print(f'CSV written: {csv_path} ({len(rows)} rows)')\n",
        "\n",
        "# Uncomment to run after JSONL generation\n",
        "# jsonl_to_csv(JSONL_PATH, CSV_PATH)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}